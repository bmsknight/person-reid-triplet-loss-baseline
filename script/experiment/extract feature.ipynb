{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bmsknight/anaconda3/envs/pytorch/lib/python2.7/site-packages/scikit_learn-0.20.0-py2.7-linux-x86_64.egg/sklearn/utils/fixes.py:313: FutureWarning: numpy not_equal will not check object identity in the future. The comparison did not return the same result as suggested by the identity (`is`)) and will change.\n",
      "  _nan_object_mask = _nan_object_array != _nan_object_array\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.nn.parallel import DataParallel\n",
    "\n",
    "import time\n",
    "import os.path as osp\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "from tri_loss.dataset import create_dataset\n",
    "from tri_loss.model.Model import Model\n",
    "from tri_loss.model.TripletLoss import TripletLoss\n",
    "from tri_loss.model.loss import global_loss\n",
    "\n",
    "from tri_loss.utils.utils import time_str\n",
    "from tri_loss.utils.utils import str2bool\n",
    "from tri_loss.utils.utils import tight_float_str as tfs\n",
    "from tri_loss.utils.utils import may_set_mode\n",
    "from tri_loss.utils.utils import load_state_dict\n",
    "from tri_loss.utils.utils import load_ckpt\n",
    "from tri_loss.utils.utils import save_ckpt\n",
    "from tri_loss.utils.utils import set_devices\n",
    "from tri_loss.utils.utils import AverageMeter\n",
    "from tri_loss.utils.utils import to_scalar\n",
    "from tri_loss.utils.utils import ReDirectSTD\n",
    "from tri_loss.utils.utils import set_seed\n",
    "from tri_loss.utils.utils import adjust_lr_exp\n",
    "from tri_loss.utils.utils import adjust_lr_staircase\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "path = os.path.dirname(os.path.dirname(os.path.abspath(\"/home/bmsknight/triplet/person-reid-triplet-loss-baseline/tri_loss/dataset/\")))\n",
    "sys.path.insert(0,path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ExtractFeature(object):\n",
    "    \"\"\"A function to be called in the val/test set, to extract features.\n",
    "    Args:\n",
    "    TVT: A callable to transfer images to specific device.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, TVT):\n",
    "        self.model = model\n",
    "        self.TVT = TVT\n",
    "\n",
    "    def extract(self, ims):\n",
    "        old_train_eval_model = self.model.training\n",
    "        # Set eval mode.\n",
    "        # Force all BN layers to use global mean and variance, also disable\n",
    "        # dropout.\n",
    "        self.model.eval()\n",
    "        ims = Variable(self.TVT(torch.from_numpy(ims).float()))\n",
    "        feat = self.model(ims)\n",
    "        feat = feat.data.cpu().numpy()\n",
    "        # Restore the model to its old train/eval mode.\n",
    "        self.model.train(old_train_eval_model)\n",
    "        return feat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(load_model_weight=True):\n",
    "    if load_model_weight:\n",
    "      if cfg.model_weight_file != '':\n",
    "        map_location = (lambda storage, loc: storage)\n",
    "        sd = torch.load(cfg.model_weight_file, map_location=map_location)\n",
    "        load_state_dict(model, sd)\n",
    "        print('Loaded model weights from {}'.format(cfg.model_weight_file))\n",
    "      else:\n",
    "        load_ckpt(modules_optims, cfg.ckpt_file)\n",
    "\n",
    "    for test_set, name in zip(test_sets, test_set_names):\n",
    "      test_set.set_feat_func(ExtractFeature(model_w, TVT))\n",
    "      print('\\n=========> Test on dataset: {} <=========\\n'.format(name))\n",
    "      test_set.eval(\n",
    "        normalize_feat=cfg.normalize_feature,\n",
    "        verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TVT, TMO = set_devices((0,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        self.resize_h_w = (256,128)\n",
    "        self.scale_im = True\n",
    "        self.im_mean = [0.486, 0.459, 0.408]\n",
    "        self.im_std = [0.229, 0.224, 0.225]\n",
    "        self.device_id = (0,)\n",
    "        self.model_weight_file = '/home/bmsknight/triplet/person-reid-triplet-loss-baseline/data/market/model_weight.pth'\n",
    "        self.margin = 0.3\n",
    "        \n",
    "        self.last_conv_stride = 1\n",
    "        self.weight_decay = 0.0005\n",
    "\n",
    "        # Initial learning rate\n",
    "        self.base_lr = 0.0002\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(last_conv_stride=cfg.last_conv_stride)\n",
    "  # Model wrapper\n",
    "model_w = DataParallel(model)\n",
    "\n",
    "#############################\n",
    "# Criteria and Optimizers   #\n",
    "#############################\n",
    "\n",
    "tri_loss = TripletLoss(margin=cfg.margin)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                     lr=cfg.base_lr,\n",
    "                     weight_decay=cfg.weight_decay)\n",
    "\n",
    "# Bind them together just to save some codes in the following usage.\n",
    "modules_optims = [model, optimizer]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys not found in source state_dict: \n",
      "\t base.layer2.0.bn1.num_batches_tracked\n",
      "\t base.layer4.1.bn3.num_batches_tracked\n",
      "\t base.layer4.0.bn1.num_batches_tracked\n",
      "\t base.layer3.5.bn1.num_batches_tracked\n",
      "\t base.layer3.2.bn2.num_batches_tracked\n",
      "\t base.layer1.0.bn1.num_batches_tracked\n",
      "\t base.layer1.1.bn2.num_batches_tracked\n",
      "\t base.bn1.num_batches_tracked\n",
      "\t base.layer3.1.bn3.num_batches_tracked\n",
      "\t base.layer2.0.bn3.num_batches_tracked\n",
      "\t base.layer2.0.downsample.1.num_batches_tracked\n",
      "\t base.layer4.0.bn2.num_batches_tracked\n",
      "\t base.layer2.2.bn3.num_batches_tracked\n",
      "\t base.layer3.4.bn2.num_batches_tracked\n",
      "\t base.layer3.4.bn3.num_batches_tracked\n",
      "\t base.layer1.0.bn3.num_batches_tracked\n",
      "\t base.layer1.1.bn1.num_batches_tracked\n",
      "\t base.layer4.0.bn3.num_batches_tracked\n",
      "\t base.layer1.1.bn3.num_batches_tracked\n",
      "\t base.layer3.0.bn3.num_batches_tracked\n",
      "\t base.layer2.3.bn1.num_batches_tracked\n",
      "\t base.layer3.2.bn3.num_batches_tracked\n",
      "\t base.layer3.0.bn2.num_batches_tracked\n",
      "\t base.layer4.2.bn3.num_batches_tracked\n",
      "\t base.layer2.2.bn1.num_batches_tracked\n",
      "\t base.layer3.5.bn2.num_batches_tracked\n",
      "\t base.layer3.0.downsample.1.num_batches_tracked\n",
      "\t base.layer3.0.bn1.num_batches_tracked\n",
      "\t base.layer3.1.bn2.num_batches_tracked\n",
      "\t base.layer2.3.bn2.num_batches_tracked\n",
      "\t base.layer3.5.bn3.num_batches_tracked\n",
      "\t base.layer3.3.bn3.num_batches_tracked\n",
      "\t base.layer1.0.bn2.num_batches_tracked\n",
      "\t base.layer1.2.bn3.num_batches_tracked\n",
      "\t base.layer2.0.bn2.num_batches_tracked\n",
      "\t base.layer4.2.bn2.num_batches_tracked\n",
      "\t base.layer4.1.bn2.num_batches_tracked\n",
      "\t base.layer2.1.bn2.num_batches_tracked\n",
      "\t base.layer3.2.bn1.num_batches_tracked\n",
      "\t base.layer1.2.bn1.num_batches_tracked\n",
      "\t base.layer1.2.bn2.num_batches_tracked\n",
      "\t base.layer2.2.bn2.num_batches_tracked\n",
      "\t base.layer3.1.bn1.num_batches_tracked\n",
      "\t base.layer4.1.bn1.num_batches_tracked\n",
      "\t base.layer3.4.bn1.num_batches_tracked\n",
      "\t base.layer1.0.downsample.1.num_batches_tracked\n",
      "\t base.layer2.1.bn3.num_batches_tracked\n",
      "\t base.layer3.3.bn2.num_batches_tracked\n",
      "\t base.layer3.3.bn1.num_batches_tracked\n",
      "\t base.layer2.3.bn3.num_batches_tracked\n",
      "\t base.layer4.2.bn1.num_batches_tracked\n",
      "\t base.layer4.0.downsample.1.num_batches_tracked\n",
      "\t base.layer2.1.bn1.num_batches_tracked\n",
      "Loaded model weights from /home/bmsknight/triplet/person-reid-triplet-loss-baseline/data/market/model_weight.pth\n"
     ]
    }
   ],
   "source": [
    "map_location = (lambda storage, loc: storage)\n",
    "sd = torch.load(cfg.model_weight_file, map_location=map_location)\n",
    "load_state_dict(model, sd)\n",
    "print('Loaded model weights from {}'.format(cfg.model_weight_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(nparray, order=2, axis=0):\n",
    "    \"\"\"Normalize a N-D numpy array along the specified axis.\"\"\"\n",
    "    norm = np.linalg.norm(nparray, ord=order, axis=axis, keepdims=True)\n",
    "    return nparray / (norm + np.finfo(np.float32).eps)\n",
    "\n",
    "\n",
    "def compute_dist(array1, array2, type='euclidean'):\n",
    "\n",
    "    if type == 'cosine':\n",
    "        array1 = normalize(array1, axis=1)\n",
    "        array2 = normalize(array2, axis=1)\n",
    "        dist = np.matmul(array1, array2.T)\n",
    "        return dist\n",
    "    else:\n",
    "        # shape [m1, 1]\n",
    "        square1 = np.sum(np.square(array1), axis=1)[..., np.newaxis]\n",
    "        # shape [1, m2]\n",
    "        square2 = np.sum(np.square(array2), axis=1)[np.newaxis, ...]\n",
    "        squared_dist = - 2 * np.matmul(array1, array2.T) + square1 + square2\n",
    "        squared_dist[squared_dist < 0] = 0\n",
    "        dist = np.sqrt(squared_dist)\n",
    "        return dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_im(cfg, im):\n",
    "    \"\"\"Pre-process image.\n",
    "    `im` is a numpy array with shape [H, W, 3], e.g. the result of\n",
    "    matplotlib.pyplot.imread(some_im_path), or\n",
    "    numpy.asarray(PIL.Image.open(some_im_path)).\"\"\"\n",
    "\n",
    "    \n",
    "    # Resize.\n",
    "    if (cfg.resize_h_w is not None) \\\n",
    "        and (cfg.resize_h_w != (im.shape[0], im.shape[1])):\n",
    "      im = cv2.resize(im, cfg.resize_h_w[::-1], interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # scaled by 1/255.\n",
    "    if cfg.scale_im:\n",
    "      im = im / 255.\n",
    "\n",
    "    # Subtract mean and scaled by std\n",
    "    # im -= np.array(self.im_mean) # This causes an error:\n",
    "    # Cannot cast ufunc subtract output from dtype('float64') to\n",
    "    # dtype('uint8') with casting rule 'same_kind'\n",
    "    if cfg.im_mean is not None:\n",
    "      im = im - np.array(cfg.im_mean)\n",
    "    if cfg.im_mean is not None and cfg.im_std is not None:\n",
    "      im = im / np.array(cfg.im_std).astype(float)\n",
    "\n",
    "    # May mirror image.\n",
    "    \n",
    "\n",
    "    # The original image has dims 'HWC', transform it to 'CHW'.\n",
    "    im = im.transpose(2, 0, 1)\n",
    "\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#im1 = im1.transpose(2,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = ExtractFeature(model=model_w,TVT=TVT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "im1 = np.asarray(PIL.Image.open(\"/home/bmsknight/triplet/person-reid-triplet-loss-baseline/data/ourdata/Database/query/priyan5.jpg\"))\n",
    "im2 = np.asarray(PIL.Image.open(\"/home/bmsknight/triplet/person-reid-triplet-loss-baseline/data/ourdata/Database/Dynamic_Database/piriyanthan/piriyanthan_26_3.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "im1 = pre_process_im(cfg, im1)\n",
    "im2 = pre_process_im(cfg, im2)\n",
    "im1 = np.reshape(im1,(1,3,256,128))\n",
    "im2 = np.reshape(im2,(1,3,256,128))\n",
    "ft1 = ext.extract(im1)\n",
    "ft2 = ext.extract(im2)\n",
    "val = compute_dist(ft1,ft2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.61921597]], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['piriyanthan', 'piriyanthan', 'piriyanthan', 'piriyanthan', 'piriyanthan', 'piriyanthan']\n",
      "['thivakaran', 'thivakaran', 'thivakaran', 'thivakaran', 'thivakaran']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bmsknight/anaconda3/envs/pytorch/lib/python2.7/site-packages/ipykernel_launcher.py:19: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['madhushan', 'madhushan', 'madhushan', 'madhushan', 'madhushan', 'madhushan', 'madhushan', 'madhushan', 'madhushan', 'madhushan']\n",
      "['harishanth', 'harishanth', 'harishanth', 'harishanth', 'harishanth']\n",
      "['athavan', 'athavan', 'athavan', 'athavan', 'athavan', 'athavan', 'athavan']\n"
     ]
    }
   ],
   "source": [
    "root_directory = '/home/bmsknight/triplet/person-reid-triplet-loss-baseline/data/ourdata/Database/Dynamic_Database/'\n",
    "folder_list = os.listdir(root_directory)\n",
    "\n",
    "\n",
    "global_feature_list = None\n",
    "name_list = []\n",
    "for name in folder_list:\n",
    "    glist = []\n",
    "    \n",
    "    directory = root_directory + name + '/'\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        input_image = np.asarray(PIL.Image.open(directory+filename))\n",
    "        input_image = pre_process_im(cfg, input_image)\n",
    "        input_image = np.reshape(input_image,(1,3,256,128))\n",
    "        feature = ext.extract(input_image)\n",
    "        glist.append(feature)\n",
    "    glist = np.asarray(glist).reshape(-1,2048)\n",
    "    if global_feature_list ==None:\n",
    "        global_feature_list = glist\n",
    "    else:\n",
    "        global_feature_list= np.vstack((global_feature_list, glist))\n",
    "    print ([name] * glist.shape[0])\n",
    "    name_list = name_list + ([name] * glist.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "querry_image = np.asarray(PIL.Image.open(\"/home/bmsknight/triplet/person-reid-triplet-loss-baseline/data/ourdata/Database/query/person1_frameNo_20.jpg\"))\n",
    "querry_image = pre_process_im(cfg, querry_image)\n",
    "querry_image = np.reshape(querry_image,(1,3,256,128))\n",
    "querry_feature = ext.extract(querry_image)\n",
    "min_dist_list = {}\n",
    "\n",
    "dist = compute_dist(global_feature_list, querry_feature, type='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thivakaran'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_list[np.argmax(dist)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.06824015,  0.20392846,  0.1027478 , ...,  0.04803802,\n",
       "          0.11008131,  0.08103167]], dtype=float32),\n",
       " array([[ 0.06824015,  0.20392846,  0.1027478 , ...,  0.04803802,\n",
       "          0.11008131,  0.08103167]], dtype=float32),\n",
       " array([[ 0.06824015,  0.20392846,  0.1027478 , ...,  0.04803802,\n",
       "          0.11008131,  0.08103167]], dtype=float32),\n",
       " array([[ 0.06824015,  0.20392846,  0.1027478 , ...,  0.04803802,\n",
       "          0.11008131,  0.08103167]], dtype=float32),\n",
       " array([[ 0.06824015,  0.20392846,  0.1027478 , ...,  0.04803802,\n",
       "          0.11008131,  0.08103167]], dtype=float32)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "priyan7.jpg piriyanthan piriyanthan\n",
      "person3_frameNo_240.jpg athavan athavan\n",
      "priyan16.jpg madhushan madhushan\n",
      "person0_frameNo_240.jpg harishanth harishanth\n",
      "person0_frameNo_220.jpg harishanth harishanth\n",
      "person0_frameNo_180.jpg harishanth harishanth\n",
      "person0_frameNo_260.jpg harishanth harishanth\n",
      "person0_frameNo_80.jpg harishanth harishanth\n",
      "person3_frameNo_340.jpg athavan athavan\n",
      "person2_frameNo_40.jpg thivakaran thivakaran\n",
      "person4_frameNo_160.jpg thivakaran thivakaran\n",
      "person2_frameNo_280.jpg thivakaran thivakaran\n",
      "person2_frameNo_240.jpg thivakaran thivakaran\n",
      "person0_frameNo_160.jpg harishanth harishanth\n",
      "person4_frameNo_20.jpg thivakaran thivakaran\n",
      "person4_frameNo_140.jpg thivakaran thivakaran\n",
      "person4_frameNo_280.jpg athavan athavan\n",
      "person4_frameNo_200.jpg thivakaran thivakaran\n",
      "person4_frameNo_60.jpg thivakaran thivakaran\n",
      "person2_frameNo_140.jpg thivakaran thivakaran\n",
      "person3_frameNo_320.jpg athavan athavan\n",
      "person2_frameNo_340.jpg thivakaran thivakaran\n",
      "person0_frameNo_380.jpg harishanth harishanth\n",
      "person5_frameNo_320.jpg thivakaran thivakaran\n",
      "priyan12.jpg piriyanthan piriyanthan\n",
      "person0_frameNo_200.jpg harishanth harishanth\n",
      "person3_frameNo_120.jpg athavan athavan\n",
      "person1_frameNo_380.jpg madhushan madhushan\n",
      "person3_frameNo_360.jpg athavan athavan\n",
      "person0_frameNo_20.jpg harishanth harishanth\n",
      "person0_frameNo_280.jpg harishanth harishanth\n",
      "priyan10.jpg piriyanthan piriyanthan\n",
      "person0_frameNo_120.jpg harishanth harishanth\n",
      "person6_frameNo_320.jpg thivakaran thivakaran\n",
      "person3_frameNo_60.jpg athavan athavan\n",
      "person5_frameNo_260.jpg thivakaran thivakaran\n",
      "priyan11.jpg piriyanthan madhushan\n",
      "priyan3.jpg piriyanthan piriyanthan\n",
      "priyan9.jpg piriyanthan harishanth\n",
      "person2_frameNo_160.jpg piriyanthan piriyanthan\n",
      "person3_frameNo_100.jpg athavan athavan\n",
      "person2_frameNo_80.jpg thivakaran thivakaran\n",
      "person2_frameNo_260.jpg thivakaran thivakaran\n",
      "priyan6.jpg piriyanthan piriyanthan\n",
      "priyan1.jpg piriyanthan madhushan\n",
      "person2_frameNo_300.jpg thivakaran thivakaran\n",
      "person0_frameNo_40.jpg harishanth harishanth\n",
      "person3_frameNo_260.jpg athavan athavan\n",
      "person3_frameNo_160.jpg athavan athavan\n",
      "person3_frameNo_140.jpg athavan athavan\n",
      "person2_frameNo_60.jpg thivakaran thivakaran\n",
      "person3_frameNo_200.jpg athavan athavan\n",
      "person3_frameNo_180.jpg athavan athavan\n",
      "priyan15.jpg piriyanthan piriyanthan\n",
      "person4_frameNo_240.jpg thivakaran thivakaran\n",
      "person2_frameNo_320.jpg thivakaran thivakaran\n",
      "person3_frameNo_80.jpg athavan athavan\n",
      "person0_frameNo_320.jpg harishanth harishanth\n",
      "person3_frameNo_20.jpg athavan athavan\n",
      "person5_frameNo_180.jpg thivakaran thivakaran\n",
      "priyan0.jpg piriyanthan piriyanthan\n",
      "person3_frameNo_300.jpg athavan athavan\n",
      "hari.jpg harishanth harishanth\n",
      "person2_frameNo_20.jpg piriyanthan piriyanthan\n",
      "person3_frameNo_40.jpg athavan athavan\n",
      "person2_frameNo_360.jpg thivakaran thivakaran\n",
      "person1_frameNo_20.jpg thivakaran thivakaran\n",
      "priyan8.jpg piriyanthan madhushan\n",
      "priyan13.jpg piriyanthan piriyanthan\n",
      "person1_frameNo_160.jpg thivakaran thivakaran\n",
      "person4_frameNo_80.jpg thivakaran thivakaran\n",
      "person4_frameNo_100.jpg thivakaran thivakaran\n",
      "person2_frameNo_120.jpg thivakaran thivakaran\n",
      "person0_frameNo_100.jpg harishanth harishanth\n",
      "person4_frameNo_120.jpg thivakaran thivakaran\n",
      "person0_frameNo_140.jpg harishanth harishanth\n",
      "person4_frameNo_40.jpg thivakaran thivakaran\n",
      "person0_frameNo_60.jpg harishanth harishanth\n",
      "person2_frameNo_100.jpg thivakaran thivakaran\n",
      "priyan5.jpg piriyanthan piriyanthan\n",
      "person2_frameNo_380.jpg thivakaran thivakaran\n",
      "person0_frameNo_300.jpg harishanth harishanth\n",
      "person3_frameNo_380.jpg athavan athavan\n",
      "person0_frameNo_360.jpg harishanth harishanth\n",
      "person2_frameNo_200.jpg thivakaran thivakaran\n",
      "person4_frameNo_360.jpg thivakaran thivakaran\n",
      "priyan2.jpg piriyanthan piriyanthan\n",
      "priyan4.jpg piriyanthan piriyanthan\n",
      "person3_frameNo_220.jpg athavan athavan\n",
      "priyan14.jpg piriyanthan madhushan\n",
      "person2_frameNo_220.jpg thivakaran thivakaran\n",
      "person5_frameNo_280.jpg thivakaran thivakaran\n",
      "person2_frameNo_180.jpg thivakaran thivakaran\n",
      "person6_frameNo_100.jpg thivakaran thivakaran\n"
     ]
    }
   ],
   "source": [
    "querrypath = \"/home/bmsknight/triplet/person-reid-triplet-loss-baseline/data/ourdata/Database/query/\"\n",
    "for filename in os.listdir(querrypath):\n",
    "    querry_image = np.asarray(PIL.Image.open(querrypath+filename))\n",
    "    querry_image = pre_process_im(cfg, querry_image)\n",
    "    querry_image = np.reshape(querry_image,(1,3,256,128))\n",
    "    querry_feature = ext.extract(querry_image)\n",
    "    \n",
    "    cos_dist = compute_dist(global_feature_list, querry_feature, type='cosine')\n",
    "    cos_pred = name_list[np.argmax(cos_dist)]\n",
    "    \n",
    "    euc_dist = compute_dist(global_feature_list, querry_feature, type='euclidean')\n",
    "    euc_pred = name_list[np.argmin(euc_dist)]\n",
    "    \n",
    "    print(filename, cos_pred, euc_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
